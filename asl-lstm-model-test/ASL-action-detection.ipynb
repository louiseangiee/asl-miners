{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a327980-4256-423c-acbe-120d68b47a7a",
   "metadata": {},
   "source": [
    "https://github.com/nicknochnack/ActionDetectionforSignLanguage/blob/main/Action%20Detection%20Refined.ipynb - REFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e45a5-ddf0-46ad-8746-f9a8bab00e42",
   "metadata": {},
   "source": [
    "# 1. IMPORT AND INSTALL DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada844c6-f5e0-4181-b11a-7746ab5d7672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.12.0\n",
      "  Obtaining dependency information for tensorflow==2.12.0 from https://files.pythonhosted.org/packages/bc/da/9bfe1212d1f350d077c5912fc2447d7e9c182b991eab58b2e203d33424dc/tensorflow-2.12.0-cp38-cp38-macosx_10_15_x86_64.whl.metadata\n",
      "  Using cached tensorflow-2.12.0-cp38-cp38-macosx_10_15_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: opencv-python in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (4.5.3.56)\n",
      "Requirement already satisfied: mediapipe in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (0.8.8)\n",
      "Requirement already satisfied: scikit-learn in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: matplotlib in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (3.3.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (24.3.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.10.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (0.4.13)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (16.0.6)\n",
      "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
      "  Obtaining dependency information for numpy<1.24,>=1.22 from https://files.pythonhosted.org/packages/d2/55/b9b4bfb9d1d828d7d3192c4059e7b4a7d755ba2e1618089af4be77c152d1/numpy-1.23.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached numpy-1.23.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (20.9)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/f3/bf/26deba06a4c910a85f78245cac7698f67cedd7efe00d04f6b3e1b3506a59/protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.12.0) (0.34.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from mediapipe) (20.3.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from mediapipe) (4.5.3.56)\n",
      "Requirement already satisfied: wheel in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from mediapipe) (0.36.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (7.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.6->jax>=0.3.15->tensorflow==2.12.0) (3.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2021.10.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/tiarahimawan/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.1)\n",
      "Using cached tensorflow-2.12.0-cp38-cp38-macosx_10_15_x86_64.whl (230.1 MB)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.12.0 opencv-python mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138c9a1d-6996-4b7c-97f5-2b8c788d10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename, askdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a46a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "494a4260-3e26-4cce-b527-74e7c6585b91",
   "metadata": {},
   "source": [
    "# 2. KEYPOINTS USING MP HOLISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b35c823-04fa-41d1-bfd6-b345b65e57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f31d9a3-19da-4d42-9c2c-9560d886cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Color conversion BGR to RGB\n",
    "    image.flags.writeable = False                   # Image is no longer writeable\n",
    "    results = model.process(image)                  # Make prediction\n",
    "    image.flags.writeable = True                    # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Color conversion RGB to BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54cde807-c1e2-4937-9d8f-e76122f8d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    if results.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "        )\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "        )\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "        )\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232a486d-bacd-4e78-b24b-81acf7e985ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Check and draw face landmarks with styling\n",
    "    if results.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "        )\n",
    "    # Check and draw pose landmarks with styling\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "        )\n",
    "    # Check and draw left hand landmarks with styling\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "        )\n",
    "    # Check and draw right hand landmarks with styling\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ba22ba-b75b-4928-a488-67b8c9badd24",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-r0utbq5z/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ee7cf36d7351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Make detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmediapipe_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0ae5e1192411>\u001b[0m in \u001b[0;36mmediapipe_detection\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmediapipe_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Color conversion BGR to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m                   \u001b[0;31m# Image is no longer writeable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# Make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m                    \u001b[0;31m# Image is now writeable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-r0utbq5z/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model, STOP THIS CELL when you're done and want to capture\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bc23aa-c7c2-483a-bff0-77c5eeac4774",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1b5405974159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdraw_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089f0c3a-c18e-4e53-a410-1c19a3c8ff88",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-r0utbq5z/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ec06103f2177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-r0utbq5z/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b142bb-aa24-4c0a-b317-fae6972e6643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59addd9a-f8fa-4982-8882-618728f2344e",
   "metadata": {},
   "source": [
    "# 3. EXTRACT KEYPOINT VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f38a51-2042-409f-95a0-852bceefd1b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1606bd8cd46c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_hand_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#this line of code actually depends on which hand youre holding up in the frame frozen, if no left hand, value = nonetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "len(results.left_hand_landmarks.landmark)\n",
    "\n",
    "#this line of code actually depends on which hand youre holding up in the frame frozen, if no left hand, value = nonetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "623d9969-1d84-437d-b3ff-9cf976eea29f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-beea7e0dea11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisibility\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43002933-865f-4f49-878b-b950102f3954",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c23279c7a4c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisibility\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m132\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_landmarks\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1404\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_hand_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_hand_landmarks\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_hand_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_hand_landmarks\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21009ffb-974f-48c9-862a-c6940886fb97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-36793951a4de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_landmarks\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1404\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0e70a0a-a509-455b-8639-33992f3dc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a039f1d-8dda-4559-8063-2bad86e33ac2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-32382b642b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4942fc06-abe3-4a9a-bbd8-c81b234d4485",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-07ec25efd29f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result_test' is not defined"
     ]
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec0f9bbd-eb69-425b-a27c-7b07ec3c1061",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f819f33bd60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result_test' is not defined"
     ]
    }
   ],
   "source": [
    "np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5831a930-0674-460b-8fbc-6580dfc428fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e335b5dd3f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '0.npy'"
     ]
    }
   ],
   "source": [
    "np.load('0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3741fc-199f-4725-afc4-ae1157dd66f8",
   "metadata": {},
   "source": [
    "# 4. SETUP FOLDERS FOR COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef947ce6-ca60-4b80-8fea-7603db3913f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that we try to detect - edit this array to alphabets/phrases\n",
    "actions = np.array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80cab76e-f96c-4aa6-b61b-5c4ee9d76e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory '/Users/tiarahimawan/Documents/GitHub/asl-miners/asl-lstm-model-test/MP_Data' was created.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path for the MP_Data directory relative to the current directory (to store the dataset)\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'MP_Data')\n",
    "\n",
    "# Check if the directory already exists. If not, create it.\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "    message = f\"Directory '{DATA_PATH}' was created.\"\n",
    "else:\n",
    "    message = f\"Directory '{DATA_PATH}' already exists.\"\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfcc110a-4c6d-478e-afe8-f1688984616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Thirty videos worth of data - adjust accordingly\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30  # Starting index for the new folders to be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91bdd29e-ac18-4480-966d-ac0cad4faf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure DATA_PATH exists\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "for action in actions:\n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    \n",
    "    # Check if the action directory exists, create it if it doesn't\n",
    "    if not os.path.exists(action_path):\n",
    "        os.makedirs(action_path)\n",
    "    \n",
    "    # List directories, assuming they are named with integers, and filter out non-digit names\n",
    "    existing_dirs = [d for d in os.listdir(action_path) if d.isdigit()]\n",
    "    \n",
    "    # Find the maximum directory number if there are any directories present, else use start_folder\n",
    "    dirmax = max([int(d) for d in existing_dirs], default=start_folder)\n",
    "    \n",
    "    # Create new directories starting from dirmax + 1\n",
    "    for sequence in range(1, no_sequences + 1):\n",
    "        new_dir = os.path.join(action_path, str(dirmax + sequence))\n",
    "        os.makedirs(new_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad89db7-fbb3-4a04-a66f-022b93541623",
   "metadata": {},
   "source": [
    "# 5A. COLLECT KEYPOINT VALUES FOR TRAINING AND TESTING (live feed)\n",
    "\n",
    "PRESS P TO PAUSE, any key to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "818c96a6-0552-4231-8fe5-d954eeeca0dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-r0utbq5z/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-018711b44752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;31m# Make detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmediapipe_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;31m# Draw landmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0ae5e1192411>\u001b[0m in \u001b[0;36mmediapipe_detection\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmediapipe_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Color conversion BGR to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m                   \u001b[0;31m# Image is no longer writeable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# Make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m                    \u001b[0;31m# Image is now writeable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-r0utbq5z/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(start_folder+1, start_folder+no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break or pause gracefully\n",
    "                key = cv2.waitKey(10)\n",
    "                if key & 0xFF == ord('q'):\n",
    "                    break\n",
    "                elif key & 0xFF == ord('p'):  # 'p' key for pause\n",
    "                    cv2.waitKey(-1)  # wait until any key is pressed\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eee17a-4012-441f-b3b7-23419b75add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd7e60-ffd3-44eb-899c-ab64662b76bd",
   "metadata": {},
   "source": [
    "# 5B. COLLECT KEYPOINT VALUES FOR TRAINING AND TESTING (From Video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0bc9fc8-2d06-46e6-977c-c3614344df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prompt the user to select a video file\n",
    "def get_video_file():\n",
    "    Tk().withdraw()  # we don't want a full GUI, so keep the root window from appearing\n",
    "    filepath = askopenfilename()  # show an \"Open\" dialog box and return the path to the selected file\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05ecbb65-ff95-486d-a0d2-0586581e49c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected video file for action '1', Sequence number: 31\n",
      "File selection cancelled. Skipping to next sequence...\n",
      "Selected video file for action '1', Sequence number: 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, start_folder \u001b[38;5;241m+\u001b[39m no_sequences \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected video file for action \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Sequence number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     video_file \u001b[38;5;241m=\u001b[39m \u001b[43mget_video_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Check if the user canceled the selection\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m video_file:\n",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m, in \u001b[0;36mget_video_file\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_video_file\u001b[39m():\n\u001b[0;32m      3\u001b[0m     Tk()\u001b[38;5;241m.\u001b[39mwithdraw()  \u001b[38;5;66;03m# we don't want a full GUI, so keep the root window from appearing\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m \u001b[43maskopenfilename\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# show an \"Open\" dialog box and return the path to the selected file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filepath\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\tkinter\\filedialog.py:384\u001b[0m, in \u001b[0;36maskopenfilename\u001b[1;34m(**options)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maskopenfilename\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk for a filename to open\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOpen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\tkinter\\commondialog.py:45\u001b[0m, in \u001b[0;36mDialog.show\u001b[1;34m(self, **options)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test_callback(master)  \u001b[38;5;66;03m# The function below is replaced for some tests.\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mmaster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fixresult(master, s)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function definitions for mediapipe_detection, draw_styled_landmarks, and extract_keypoints need to be here\n",
    "#STOP THIS CELL TO QUIT\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos for each action\n",
    "        for sequence in range(start_folder + 1, start_folder + no_sequences + 1):\n",
    "            print(f\"Selected video file for action '{action}', Sequence number: {sequence}\")\n",
    "            video_file = get_video_file()\n",
    "            # Check if the user canceled the selection\n",
    "            if not video_file:\n",
    "                print(\"File selection cancelled. Skipping to next sequence...\")\n",
    "                continue  # Skip the rest of the loop and prompt for the next file\n",
    "            \n",
    "            print(f\"Selected video file for action '{action}', Sequence number: {sequence} is: {video_file}\")\n",
    "            \n",
    "            cap = cv2.VideoCapture(video_file)\n",
    "            \n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Failed to grab frame. Exiting this sequence...\")\n",
    "                    break\n",
    "                \n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                if frame_num == 0:\n",
    "                    display_text = f'STARTING COLLECTION for {action} Video Number {sequence}'\n",
    "                else:\n",
    "                    display_text = f'Collecting frames for {action} Video Number {sequence}'\n",
    "                \n",
    "                cv2.putText(image, display_text, (15, 12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                key = cv2.waitKey(10)\n",
    "                if key & 0xFF == ord('q'):\n",
    "                    break\n",
    "                elif key & 0xFF == ord('p'):\n",
    "                    cv2.waitKey(-1)  # Pause until any key is pressed\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            # If the user presses 'q', break out of all loops\n",
    "            if key & 0xFF == ord('q'):\n",
    "                print(\"Exiting data collection...\")\n",
    "                break\n",
    "        # If the user presses 'q', break out of the action loop as well\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ea1aa",
   "metadata": {},
   "source": [
    "# COLLECT TRAINING DATA (NUMPY FILES) and DATA PRE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = os.listdir('asl-lstm-model-test/asl_dataset')\n",
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    action_path = os.path.join('asl-lstm-model-test/asl_dataset', action)\n",
    "    if not os.path.isdir(action_path):\n",
    "        continue\n",
    "    sequence_files = os.listdir(action_path)\n",
    "    for sequence_file in sequence_files:\n",
    "        if sequence_file.endswith('.npy'):\n",
    "            path_to_sequence = os.path.join(action_path, sequence_file)\n",
    "            sequence = np.load(path_to_sequence)\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label_map[action])\n",
    "\n",
    "# Now, sequences is a list of numpy arrays with the keypoints from each .npy file\n",
    "# and labels is a list of the corresponding labels for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8065ca2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-44a76664e9b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# One-hot encode the labels if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# One-hot encode the labels if necessary\n",
    "y = to_categorical(y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4948a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e21395-5d94-4c20-8f35-f56c9173f773",
   "metadata": {},
   "source": [
    "# 6. PREPROCESS DATA AND CREATE LABELS AND FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83329f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1b17f-3f5c-4f7e-b65d-dce47bf8c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb92a9cf-7cdd-4d83-b5d6-f4851b83ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6dc71fb-182e-46a9-81a8-dda831049f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0,\n",
       " '2': 1,\n",
       " '3': 2,\n",
       " '4': 3,\n",
       " '5': 4,\n",
       " '6': 5,\n",
       " '7': 6,\n",
       " '8': 7,\n",
       " '9': 8,\n",
       " '10': 9}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "068e8a08-5f88-4b08-bbdb-8ec6c55258e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a27a3dac-d9cb-495b-98b9-9df32607a3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 30, 1662)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9a07b3-e235-441e-9ba6-081b6860aca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2515,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6169a8b4-f397-488d-a097-520767703bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f61e347-8b18-4247-ad49-767b32e6058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2515, 63)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134debe-a93b-457a-8b10-4ffc5d4fc98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2abc1c0-4209-4f38-a1ba-132391fea385",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df4f790c-85fc-42ae-8e26-7c29b368f862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909562a9-d469-4321-95b9-8afd3481779e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7daa248f-8672-4e34-a851-194e12143838",
   "metadata": {},
   "source": [
    "# 7. BUILD AND TRAIN LSTM NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e03e118d-b567-47e8-a617-ddd51b06d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "892210e2-dcef-4d1e-9413-5e542755f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9324ec6-41c4-451e-a923-934e5b3cd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9e30ce4-5a27-4f32-85fb-9ced8bd009d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd26791a-5538-440a-ad77-41d879806765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "9/9 [==============================] - 5s 41ms/step - loss: 2.5962 - categorical_accuracy: 0.1263\n",
      "Epoch 2/2000\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 2.3057 - categorical_accuracy: 0.0807\n",
      "Epoch 3/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3025 - categorical_accuracy: 0.1018\n",
      "Epoch 4/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3018 - categorical_accuracy: 0.1018\n",
      "Epoch 5/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.0877\n",
      "Epoch 6/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2995 - categorical_accuracy: 0.1088\n",
      "Epoch 7/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3152 - categorical_accuracy: 0.1263\n",
      "Epoch 8/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2800 - categorical_accuracy: 0.1018\n",
      "Epoch 9/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.4599 - categorical_accuracy: 0.1123\n",
      "Epoch 10/2000\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 2.3009 - categorical_accuracy: 0.1298\n",
      "Epoch 11/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2961 - categorical_accuracy: 0.1123\n",
      "Epoch 12/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.2956 - categorical_accuracy: 0.1018\n",
      "Epoch 13/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2905 - categorical_accuracy: 0.1123\n",
      "Epoch 14/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2889 - categorical_accuracy: 0.1088\n",
      "Epoch 15/2000\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 2.2896 - categorical_accuracy: 0.1123\n",
      "Epoch 16/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2910 - categorical_accuracy: 0.1193\n",
      "Epoch 17/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2919 - categorical_accuracy: 0.1088\n",
      "Epoch 18/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2903 - categorical_accuracy: 0.1193\n",
      "Epoch 19/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2946 - categorical_accuracy: 0.1193\n",
      "Epoch 20/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2920 - categorical_accuracy: 0.1193\n",
      "Epoch 21/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2873 - categorical_accuracy: 0.1158\n",
      "Epoch 22/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.2864 - categorical_accuracy: 0.1158\n",
      "Epoch 23/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2793 - categorical_accuracy: 0.0982\n",
      "Epoch 24/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2750 - categorical_accuracy: 0.1228\n",
      "Epoch 25/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.2832 - categorical_accuracy: 0.1088\n",
      "Epoch 26/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2836 - categorical_accuracy: 0.1053\n",
      "Epoch 27/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3187 - categorical_accuracy: 0.1123\n",
      "Epoch 28/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2938 - categorical_accuracy: 0.1123\n",
      "Epoch 29/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.2960 - categorical_accuracy: 0.1123\n",
      "Epoch 30/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2939 - categorical_accuracy: 0.1123\n",
      "Epoch 31/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2896 - categorical_accuracy: 0.1123\n",
      "Epoch 32/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2853 - categorical_accuracy: 0.1123\n",
      "Epoch 33/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2835 - categorical_accuracy: 0.1123\n",
      "Epoch 34/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2809 - categorical_accuracy: 0.1158\n",
      "Epoch 35/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2837 - categorical_accuracy: 0.1158\n",
      "Epoch 36/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2784 - categorical_accuracy: 0.1158\n",
      "Epoch 37/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2749 - categorical_accuracy: 0.1193\n",
      "Epoch 38/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2745 - categorical_accuracy: 0.1158\n",
      "Epoch 39/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2586 - categorical_accuracy: 0.1193\n",
      "Epoch 40/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2964 - categorical_accuracy: 0.1333\n",
      "Epoch 41/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2791 - categorical_accuracy: 0.1088\n",
      "Epoch 42/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2772 - categorical_accuracy: 0.1088\n",
      "Epoch 43/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.2750 - categorical_accuracy: 0.1088\n",
      "Epoch 44/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3032 - categorical_accuracy: 0.1018\n",
      "Epoch 45/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2999 - categorical_accuracy: 0.1053\n",
      "Epoch 46/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3011 - categorical_accuracy: 0.1053\n",
      "Epoch 47/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2993 - categorical_accuracy: 0.1053\n",
      "Epoch 48/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2981 - categorical_accuracy: 0.1053\n",
      "Epoch 49/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2943 - categorical_accuracy: 0.0982\n",
      "Epoch 50/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2939 - categorical_accuracy: 0.1158\n",
      "Epoch 51/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2925 - categorical_accuracy: 0.1228\n",
      "Epoch 52/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 8.3842 - categorical_accuracy: 0.0982\n",
      "Epoch 53/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2937 - categorical_accuracy: 0.0807\n",
      "Epoch 54/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.4668 - categorical_accuracy: 0.1158\n",
      "Epoch 55/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3024 - categorical_accuracy: 0.1018\n",
      "Epoch 56/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3019 - categorical_accuracy: 0.1018\n",
      "Epoch 57/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 5.0690 - categorical_accuracy: 0.1053\n",
      "Epoch 58/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2938 - categorical_accuracy: 0.1018\n",
      "Epoch 59/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3541 - categorical_accuracy: 0.0947\n",
      "Epoch 60/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3012 - categorical_accuracy: 0.1158\n",
      "Epoch 61/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3005 - categorical_accuracy: 0.1193\n",
      "Epoch 62/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2933 - categorical_accuracy: 0.1193\n",
      "Epoch 63/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.2747 - categorical_accuracy: 0.1263\n",
      "Epoch 64/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3729 - categorical_accuracy: 0.1263\n",
      "Epoch 65/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3370 - categorical_accuracy: 0.1018\n",
      "Epoch 66/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3518 - categorical_accuracy: 0.1193\n",
      "Epoch 67/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.5402 - categorical_accuracy: 0.1053\n",
      "Epoch 68/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.6361 - categorical_accuracy: 0.1088\n",
      "Epoch 69/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.8452 - categorical_accuracy: 0.1158\n",
      "Epoch 70/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 9.6085 - categorical_accuracy: 0.1123\n",
      "Epoch 71/2000\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 58.2536 - categorical_accuracy: 0.1158\n",
      "Epoch 72/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 19.6458 - categorical_accuracy: 0.1018\n",
      "Epoch 73/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 11.1996 - categorical_accuracy: 0.0982\n",
      "Epoch 74/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.6510 - categorical_accuracy: 0.1088\n",
      "Epoch 75/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.6789 - categorical_accuracy: 0.0982\n",
      "Epoch 76/2000\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 3.1492 - categorical_accuracy: 0.0877\n",
      "Epoch 77/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 3.8401 - categorical_accuracy: 0.1193\n",
      "Epoch 78/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 11.4274 - categorical_accuracy: 0.1123\n",
      "Epoch 79/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 10.2966 - categorical_accuracy: 0.1088\n",
      "Epoch 80/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 10.9823 - categorical_accuracy: 0.1158\n",
      "Epoch 81/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 37.7855 - categorical_accuracy: 0.0877\n",
      "Epoch 82/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 35.0201 - categorical_accuracy: 0.0947\n",
      "Epoch 83/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 4.0441 - categorical_accuracy: 0.1053\n",
      "Epoch 84/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 88.5488 - categorical_accuracy: 0.1018\n",
      "Epoch 85/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.2983 - categorical_accuracy: 0.1053\n",
      "Epoch 86/2000\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 2.2945 - categorical_accuracy: 0.1053\n",
      "Epoch 87/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2944 - categorical_accuracy: 0.1053\n",
      "Epoch 88/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3498 - categorical_accuracy: 0.1018\n",
      "Epoch 89/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3027 - categorical_accuracy: 0.1018\n",
      "Epoch 90/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3026 - categorical_accuracy: 0.0737\n",
      "Epoch 91/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3026 - categorical_accuracy: 0.0912\n",
      "Epoch 92/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3026 - categorical_accuracy: 0.1018\n",
      "Epoch 93/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3026 - categorical_accuracy: 0.1018\n",
      "Epoch 94/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3026 - categorical_accuracy: 0.1018\n",
      "Epoch 95/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3025 - categorical_accuracy: 0.1018\n",
      "Epoch 96/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3025 - categorical_accuracy: 0.0807\n",
      "Epoch 97/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3025 - categorical_accuracy: 0.0737\n",
      "Epoch 98/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3025 - categorical_accuracy: 0.1053\n",
      "Epoch 99/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3025 - categorical_accuracy: 0.1053\n",
      "Epoch 100/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3025 - categorical_accuracy: 0.1053\n",
      "Epoch 101/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3025 - categorical_accuracy: 0.1053\n",
      "Epoch 102/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 103/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3025 - categorical_accuracy: 0.1053\n",
      "Epoch 104/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 105/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 106/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 107/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 108/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 109/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 110/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 111/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3024 - categorical_accuracy: 0.1053\n",
      "Epoch 112/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 113/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 114/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 115/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 116/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 117/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 118/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 119/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 120/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 121/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 122/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3023 - categorical_accuracy: 0.1053\n",
      "Epoch 123/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 124/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 125/2000\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 126/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 127/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 128/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 129/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 130/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 131/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 132/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 133/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 134/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 135/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 136/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 137/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 138/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 139/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3022 - categorical_accuracy: 0.1053\n",
      "Epoch 140/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 141/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 142/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 143/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 144/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 145/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 146/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 147/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 148/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 149/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 150/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 151/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 152/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 153/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 154/2000\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 155/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 156/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 157/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 158/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 159/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 160/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 161/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 162/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 163/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 164/2000\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 165/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 166/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 167/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 168/2000\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 169/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 170/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 171/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 172/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 173/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 174/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 175/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 176/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 177/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 178/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 179/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 180/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 181/2000\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 182/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 183/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 184/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 185/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 186/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 187/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 188/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 189/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 190/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 191/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 192/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 193/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 194/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 195/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 196/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 197/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 198/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 199/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 200/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 201/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 202/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 203/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 204/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 205/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 206/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 207/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 208/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 209/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 210/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 211/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 212/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 213/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3021 - categorical_accuracy: 0.1053\n",
      "Epoch 214/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 215/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 216/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 217/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 218/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 219/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 220/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 221/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 222/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 223/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 224/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 225/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 226/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 227/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 228/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 229/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 230/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 231/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 232/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 233/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 234/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 235/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 236/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 237/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 238/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 239/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 240/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 241/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 242/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 243/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 244/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 245/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 246/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 247/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 248/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 249/2000\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 250/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 251/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 252/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 253/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 254/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 255/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 256/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 257/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 258/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 259/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 260/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 261/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 262/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 263/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 264/2000\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 265/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 266/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 267/2000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 268/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 269/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 270/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 271/2000\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 272/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 273/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 274/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 275/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 276/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 277/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 278/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 279/2000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 280/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 281/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 282/2000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 283/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 284/2000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3020 - categorical_accuracy: 0.1053\n",
      "Epoch 285/2000\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.3022 - categorical_accuracy: 0.1016"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bda8d9-e532-4ccd-bb8a-c6474df47ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3865e1df-3b96-4602-b0e1-7327cfd33665",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442412fe-eea1-40ab-9684-84719d5976df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ae551-bac4-4eae-92cc-0fe85a2df82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b18dff5-bbd8-455f-9220-b759253782b8",
   "metadata": {},
   "source": [
    "# 8. MAKE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca3a9c33-6312-4c3f-ad5a-6c857f26bd9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb6f05c1-db33-4dba-a72f-1df85e89a3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92e969fe-50f4-4304-9111-6a8db1dbf292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1806a61-e9db-4a92-8e5f-d42d798e6d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23d07b21-a8e4-4494-baec-99d2b70e5807",
   "metadata": {},
   "source": [
    "# 9. SAVE WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fc82211-4c1e-4a0b-b665-26e11fa8de07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63849cf-1bd1-44c9-a137-47291c9da320",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5da1fcf0-e084-45d8-9a6c-769cbe0578e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e9f5e-0121-4430-8fff-837862159a31",
   "metadata": {},
   "source": [
    "# 10. EVALUATION USING CONFUSION MATRIX AND ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecb3756c-1347-45f5-9fd3-6f5774abc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07fb13eb-8eba-493f-8945-3b093535515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0895bdaa-412e-49c3-b280-8aa7d57cc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2e985ac-f793-410a-9802-a76d8fc7e8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[13.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[14.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[14.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[13.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[13.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[14.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[14.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[11.,  0.],\n",
       "        [ 4.,  0.]],\n",
       "\n",
       "       [[ 0., 15.],\n",
       "        [ 0.,  0.]],\n",
       "\n",
       "       [[14.,  0.],\n",
       "        [ 1.,  0.]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f444ebb-752f-4d4e-a4df-817359b8615f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab392dc-9912-4430-b895-d32a10b52ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25669149-7f3d-4974-9fdf-78c1a93ec856",
   "metadata": {},
   "source": [
    "# 11. TEST IN REAL TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6be8977e-09da-4265-9173-62b4969cec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7a59c6e-d4bd-4570-953a-f5f96e940dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62f0e5a4-5818-4858-ab1d-3fbcb26ed754",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m18\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mprob_viz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[49], line 5\u001b[0m, in \u001b[0;36mprob_viz\u001b[1;34m(res, actions, input_frame, colors)\u001b[0m\n\u001b[0;32m      3\u001b[0m output_frame \u001b[38;5;241m=\u001b[39m input_frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(res):\n\u001b[1;32m----> 5\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(output_frame, (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m60\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), (\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;241m90\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), colors[num], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(output_frame, actions[num], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m85\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_frame\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c40c3-494b-44a5-aa8f-6c47971f6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
